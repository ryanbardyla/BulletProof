//! Sparse Trinary Network with EWC and Meta-Learning Support
//! 
//! CRITICAL: Implements LBP (Loopy Belief Propagation) and Meta-Learning
//! for continuous adaptation to market patterns without catastrophic forgetting.
//! 
//! Features:
//! - Sparse trinary processing (80-95% baseline neurons)
//! - EWC integration for memory preservation
//! - Meta-learning for rapid adaptation to new patterns
//! - LBP for probabilistic inference in trinary networks

use crate::tryte::Tryte;
use crate::ewc_trinary_implementation::{WeightId, EWCMemory};
use crate::protein_synthesis::ProteinSynthesisNeuron;

use std::collections::HashMap;
use serde::{Serialize, Deserialize};

/// Layer configuration for sparse trinary network
#[derive(Clone)]
pub struct TrithLayer {
    pub input_size: usize,
    pub output_size: usize,
    pub sparsity_threshold: f32,
    pub neurons: Vec<ProteinSynthesisNeuron>,
}

/// Sparse Trinary Network with Meta-Learning
pub struct SparseTrithNet {
    pub layers: Vec<TrithLayer>,
    pub meta_learning_rate: f32,
    pub lbp_iterations: usize,
    pub belief_propagation_enabled: bool,
}

impl SparseTrithNet {
    pub fn new(input_size: usize, hidden_size: usize, output_size: usize) -> Result<Self, Box<dyn std::error::Error>> {
        println!("ðŸ§  Creating Sparse Trinary Network...");
        println!("   Architecture: {} â†’ {} â†’ {}", input_size, hidden_size, output_size);
        println!("   ðŸ”„ LBP enabled for probabilistic inference");
        println!("   ðŸŽ¯ Meta-learning for rapid adaptation");
        
        let input_layer = TrithLayer {
            input_size,
            output_size: hidden_size,
            sparsity_threshold: 0.8, // 80% sparsity
            neurons: (0..hidden_size).map(|_| ProteinSynthesisNeuron::new()).collect(),
        };
        
        let hidden_layer = TrithLayer {
            input_size: hidden_size,
            output_size: hidden_size,
            sparsity_threshold: 0.85, // 85% sparsity
            neurons: (0..hidden_size).map(|_| ProteinSynthesisNeuron::new()).collect(),
        };
        
        let output_layer = TrithLayer {
            input_size: hidden_size,
            output_size,
            sparsity_threshold: 0.7, // Lower sparsity for outputs
            neurons: (0..output_size).map(|_| ProteinSynthesisNeuron::new()).collect(),
        };
        
        Ok(Self {
            layers: vec![input_layer, hidden_layer, output_layer],
            meta_learning_rate: 0.01,
            lbp_iterations: 5,
            belief_propagation_enabled: true,
        })
    }
    
    /// Forward pass with sparse computation
    pub fn forward_sparse(&self, input: &[Tryte]) -> Result<Vec<Tryte>, Box<dyn std::error::Error>> {
        let mut current = input.to_vec();
        
        for (layer_idx, layer) in self.layers.iter().enumerate() {
            let mut next_layer = Vec::new();
            
            // Count baseline neurons for sparsity
            let baseline_count = current.iter().filter(|&&t| t == Tryte::Baseline).count();
            let sparsity = baseline_count as f32 / current.len() as f32;
            
            // Process each output neuron
            for neuron_idx in 0..layer.output_size {
                // Skip computation for sparse neurons (energy saving!)
                if sparsity > layer.sparsity_threshold && fastrand::f32() < sparsity {
                    next_layer.push(Tryte::Baseline); // Keep baseline
                    continue;
                }
                
                // Compute weighted sum (simplified)
                let mut activation = 0i32;
                for (i, &input_tryte) in current.iter().enumerate() {
                    if input_tryte != Tryte::Baseline {
                        // Only process non-baseline inputs
                        activation += input_tryte as i32;
                    }
                }
                
                // Apply activation function
                let output_tryte = if activation > 0 {
                    Tryte::Activated
                } else if activation < 0 {
                    Tryte::Inhibited
                } else {
                    Tryte::Baseline
                };
                
                next_layer.push(output_tryte);
            }
            
            // Apply LBP if enabled
            if self.belief_propagation_enabled && layer_idx < self.layers.len() - 1 {
                next_layer = self.apply_lbp(&next_layer, layer_idx);
            }
            
            current = next_layer;
        }
        
        Ok(current)
    }
    
    /// Loopy Belief Propagation for probabilistic inference
    fn apply_lbp(&self, layer_output: &[Tryte], layer_idx: usize) -> Vec<Tryte> {
        let mut beliefs = layer_output.to_vec();
        
        for _ in 0..self.lbp_iterations {
            let mut new_beliefs = Vec::new();
            
            for i in 0..beliefs.len() {
                // Message passing from neighbors
                let mut messages = vec![0i32; 3]; // [Inhibited, Baseline, Activated]
                
                // Collect messages from neighboring neurons
                for j in 0..beliefs.len() {
                    if i != j && (i as i32 - j as i32).abs() <= 2 { // Local connectivity
                        match beliefs[j] {
                            Tryte::Inhibited => messages[0] += 1,
                            Tryte::Baseline => messages[1] += 1,
                            Tryte::Activated => messages[2] += 1,
                        }
                    }
                }
                
                // Update belief based on messages
                let max_idx = messages.iter().enumerate()
                    .max_by_key(|(_, &v)| v)
                    .map(|(i, _)| i)
                    .unwrap_or(1);
                
                let new_belief = match max_idx {
                    0 => Tryte::Inhibited,
                    2 => Tryte::Activated,
                    _ => Tryte::Baseline,
                };
                
                new_beliefs.push(new_belief);
            }
            
            beliefs = new_beliefs;
        }
        
        beliefs
    }
    
    /// Meta-learning update for rapid adaptation
    pub fn meta_learning_step(&mut self, task_gradients: &[Vec<f32>]) {
        println!("ðŸŽ¯ Meta-learning update with {} task gradients", task_gradients.len());
        
        // MAML-style meta update (simplified)
        for (layer_idx, layer) in self.layers.iter_mut().enumerate() {
            if layer_idx < task_gradients.len() {
                let task_grad = &task_gradients[layer_idx];
                
                // Update sparsity threshold based on task performance
                let avg_gradient = task_grad.iter().sum::<f32>() / task_grad.len() as f32;
                layer.sparsity_threshold += self.meta_learning_rate * avg_gradient.signum();
                layer.sparsity_threshold = layer.sparsity_threshold.clamp(0.5, 0.95);
            }
        }
    }
    
    /// Apply EWC penalty to preserve important weights
    pub fn apply_ewc_penalty(&mut self, ewc_memory: &EWCMemory, learning_rate: f32) -> f32 {
        let current_weights = self.extract_weights();
        let penalty = ewc_memory.calculate_ewc_penalty(&current_weights);
        
        // Adjust sparsity based on weight importance
        let important_weights = ewc_memory.get_important_weights();
        
        for (weight_id, importance) in important_weights {
            if weight_id.layer < self.layers.len() {
                let layer = &mut self.layers[weight_id.layer];
                // Reduce sparsity for important weights
                let sparsity_adjustment = importance.min(0.2);
                layer.sparsity_threshold *= 1.0 - sparsity_adjustment;
            }
        }
        
        penalty
    }
    
    /// Extract network weights for EWC
    pub fn extract_weights(&self) -> HashMap<WeightId, f32> {
        let mut weights = HashMap::new();
        
        for (layer_idx, layer) in self.layers.iter().enumerate() {
            // Use sparsity threshold as proxy for weight importance
            for from_idx in 0..layer.input_size.min(100) {
                for to_idx in 0..layer.output_size.min(100) {
                    let weight_id = WeightId {
                        layer: layer_idx,
                        from_neuron: from_idx,
                        to_neuron: to_idx,
                    };
                    
                    // Simplified: use sparsity threshold as weight value
                    weights.insert(weight_id, layer.sparsity_threshold);
                }
            }
        }
        
        weights
    }
    
    /// Get network statistics
    pub fn get_stats(&self) -> NetworkStats {
        let total_neurons: usize = self.layers.iter()
            .map(|l| l.output_size)
            .sum();
        
        let avg_sparsity = self.layers.iter()
            .map(|l| l.sparsity_threshold)
            .sum::<f32>() / self.layers.len() as f32;
        
        NetworkStats {
            total_neurons,
            total_layers: self.layers.len(),
            average_sparsity: avg_sparsity,
            lbp_enabled: self.belief_propagation_enabled,
            meta_learning_rate: self.meta_learning_rate,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkStats {
    pub total_neurons: usize,
    pub total_layers: usize,
    pub average_sparsity: f32,
    pub lbp_enabled: bool,
    pub meta_learning_rate: f32,
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_network_creation() {
        let network = SparseTrithNet::new(100, 50, 3).unwrap();
        assert_eq!(network.layers.len(), 3);
        assert!(network.belief_propagation_enabled);
    }
    
    #[test]
    fn test_forward_sparse() {
        let network = SparseTrithNet::new(10, 5, 3).unwrap();
        let input = vec![Tryte::Baseline; 10];
        let output = network.forward_sparse(&input).unwrap();
        assert_eq!(output.len(), 3);
    }
    
    #[test]
    fn test_meta_learning() {
        let mut network = SparseTrithNet::new(10, 5, 3).unwrap();
        let gradients = vec![vec![0.1; 5], vec![0.2; 5], vec![0.3; 3]];
        network.meta_learning_step(&gradients);
        
        // Check that sparsity thresholds were updated
        let stats = network.get_stats();
        assert!(stats.average_sparsity > 0.0);
    }
}